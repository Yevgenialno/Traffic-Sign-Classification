{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torchsummary import summary\nimport time\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms,models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom PIL import Image, ImageFont, ImageDraw\nfrom pathlib import Path\nimport pandas as pd\nimport cv2\nimport requests\nfrom sklearn.metrics import f1_score\nfrom torchmetrics import F1Score, Accuracy, ConfusionMatrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport albumentations as A","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:45:50.370944Z","iopub.execute_input":"2022-08-29T07:45:50.371801Z","iopub.status.idle":"2022-08-29T07:45:51.992578Z","shell.execute_reply.started":"2022-08-29T07:45:50.371709Z","shell.execute_reply":"2022-08-29T07:45:51.991462Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"seed = 7\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\n#random.seed(seed)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda')\ndevice=torch.device(device)\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:45:52.006265Z","iopub.execute_input":"2022-08-29T07:45:52.006641Z","iopub.status.idle":"2022-08-29T07:45:52.147588Z","shell.execute_reply.started":"2022-08-29T07:45:52.006605Z","shell.execute_reply":"2022-08-29T07:45:52.146495Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nlearning_rate = 0.0001\nnumClasses = 205","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:45:52.149821Z","iopub.execute_input":"2022-08-29T07:45:52.150470Z","iopub.status.idle":"2022-08-29T07:45:52.158460Z","shell.execute_reply.started":"2022-08-29T07:45:52.150428Z","shell.execute_reply":"2022-08-29T07:45:52.157324Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class TrafficData(Dataset):\n    def __init__(self, df, image_dir, transforms=None):\n        self.image_ids = df['Path'].unique()\n        self.df = df\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index, size = [112, 112]):\n        image_path = self.image_ids[index]\n        records = self.df[self.df['Path'] == image_path]\n\n        #print(f'{self.image_dir}/{image_path}')\n        image = cv2.imread(f'{self.image_dir}/{image_path}', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, size)\n        image = image.astype(float) / 255.0\n\n        target = records['ClassId'].values\n\n        if self.transforms:\n            image = self.transforms(**image)\n\n        return image, target\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    @staticmethod\n    def create_dataset(df, dir, transform=None):\n       dataset = TrafficData(df, dir)\n       return dataset\n\n    @staticmethod \n    def loader(dataset, batch_size, shuffle=True, num_workers=0,):\n       data_loader = DataLoader(\n          dataset,\n          batch_size=batch_size,\n          shuffle=shuffle,\n          num_workers=num_workers,\n      )\n       return data_loader\n\ndata_transforms = transforms.Compose([\n    transforms.Resize([112, 112]),\n    transforms.ToTensor()\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:45:56.337991Z","iopub.execute_input":"2022-08-29T07:45:56.338389Z","iopub.status.idle":"2022-08-29T07:45:56.349230Z","shell.execute_reply.started":"2022-08-29T07:45:56.338353Z","shell.execute_reply":"2022-08-29T07:45:56.347863Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"path = Path(\"../input/traffic-signs-gtsrb-plus-162-custom-classes/Data_images\")\ndf_train = pd.read_csv(\"../input/data-info/Train_balanced_data.csv\")\ndf_test = pd.read_csv(\"../input/data-info/Test_data_cleaned.csv\")\n#df_inference = pd.read_csv('Data_images/data_inference.csv')\n\ntrain_data = TrafficData.create_dataset(df_train, path, data_transforms)\ntrain_set, val_set = torch.utils.data.random_split(train_data, [29000, 6465])\n\ntrain_data_loader = TrafficData.loader(train_set, BATCH_SIZE)\nval_data_loader = TrafficData.loader(val_set, BATCH_SIZE)\n\ntest_data = TrafficData.create_dataset(df_test, path, data_transforms)\ntest_set, _ = torch.utils.data.random_split(test_data, [10000, 31692])\ntest_data_loader = TrafficData.loader(test_set, BATCH_SIZE)\nbig_test_data_loader = TrafficData.loader(test_data, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:45:59.711366Z","iopub.execute_input":"2022-08-29T07:45:59.712242Z","iopub.status.idle":"2022-08-29T07:45:59.834115Z","shell.execute_reply.started":"2022-08-29T07:45:59.712203Z","shell.execute_reply":"2022-08-29T07:45:59.832997Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for X, y in test_data_loader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:02.733910Z","iopub.execute_input":"2022-08-29T07:46:02.734612Z","iopub.status.idle":"2022-08-29T07:46:04.136640Z","shell.execute_reply.started":"2022-08-29T07:46:02.734577Z","shell.execute_reply":"2022-08-29T07:46:04.135495Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"eff_net_b0 = models.efficientnet_b0(pretrained=True)\n#vgg16 = models.vgg16(pretrained=True)\n#ResNet = models.resnet34(pretrained=True)\n\nclass Eff_net(nn.Module):\n    def __init__(self, pretrained_model):\n        super(Eff_net,self).__init__()\n        self.Eff_net = pretrained_model\n        #num_inputs = pretrained_model.classifier[1].in_features\n        self.fl1 = nn.Sequential(nn.Linear(1000, 256), nn.ReLU())\n        self.fl2 = nn.Sequential(nn.Linear(256, numClasses), nn.Softmax())\n        \n    def forward(self, X):\n        X = self.Eff_net(X)\n        X = self.fl1(X)\n        #X = F.dropout(X, p=0.25)\n        X = self.fl2(X)\n        return X\n    \n    def pred(self, X):\n        X = cv2.resize(X, (112, 112))\n        X = np.reshape(X, (1, 112, 112, 3))\n        X = torch.tensor(X)\n        X = X.float()\n        X = X.permute(0, 3, 1, 2)\n        X = X.cuda()\n        prediction = self.forward(X)\n        prediction = torch.argmax(prediction, axis=1)\n        return prediction.cpu().numpy()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:05.928823Z","iopub.execute_input":"2022-08-29T07:46:05.929243Z","iopub.status.idle":"2022-08-29T07:46:08.163792Z","shell.execute_reply.started":"2022-08-29T07:46:05.929201Z","shell.execute_reply":"2022-08-29T07:46:08.162713Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model1 = Eff_net(eff_net_b0)\n#model2 = Eff_net(vgg16)\n#model3 = Eff_net(ResNet)\nmodel = model1\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:08.166052Z","iopub.execute_input":"2022-08-29T07:46:08.166444Z","iopub.status.idle":"2022-08-29T07:46:11.287375Z","shell.execute_reply.started":"2022-08-29T07:46:08.166406Z","shell.execute_reply":"2022-08-29T07:46:11.286332Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim = True)\n    top_pred = torch.reshape(top_pred, (-1,))\n    true_pred = (top_pred == y).float().sum()\n#     correct = top_pred.eq(y.view_as(top_pred)).sum()\n#     acc = correct.float() / y.shape[0]\n    return true_pred / len(y)\n\ndef calculate_f1_score(output,labels):\n    predictions = output.argmax(1, keepdim = True)\n    predictions = torch.reshape(predictions, (-1,))\n    f1 = F1Score(num_classes=205).to(device)\n    return f1(predictions, labels)\n\ndef show_meta(i):\n    img = cv2.imread(str(path / ('Meta/' + str(int(i)) + '.png')))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    \ndef predict_and_check(model, img):\n    label_pred = model.pred(img)\n    meta_img_bgr = cv2.imread(str(path / ('Meta/' + str(int(label_pred)) + '.png')))\n    meta_img_bgr = meta_img_bgr / 255.0\n    meta_img = np.zeros(meta_img_bgr.shape)\n    meta_img[:, :, 0] = meta_img_bgr[:, :, 2]\n    meta_img[:, :, 1] = meta_img_bgr[:, :, 1]\n    meta_img[:, :, 2] = meta_img_bgr[:, :, 0]\n    #meta_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    fig, ax = plt.subplots(ncols=2, figsize=(10, 10))\n    ax[0].imshow(img)\n    ax[0].set_title('your image')\n    ax[1].imshow(meta_img)\n    ax[1].set_title('predicted class: ' + str(int(label_pred)))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:11.291447Z","iopub.execute_input":"2022-08-29T07:46:11.291764Z","iopub.status.idle":"2022-08-29T07:46:11.303146Z","shell.execute_reply.started":"2022-08-29T07:46:11.291728Z","shell.execute_reply":"2022-08-29T07:46:11.302136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train(model, loader, opt, criterion):\n    print('training to', len(loader), end=':')\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n    \n    i = 0\n    for (images, labels) in loader:\n        print(i, end=';')\n        if i % 101 == 100:\n            print('')\n        i += 1\n        images = images.permute(0,3,1,2)\n        images = images.float()\n        labels = torch.reshape(labels, (-1,))\n        images = images.cuda()\n        labels = labels.cuda()\n        \n        opt.zero_grad()\n        \n        output = model(images)\n        loss = criterion(output, labels)\n        \n\n        loss.backward()\n        \n        acc = calculate_accuracy(output, labels)\n        \n        opt.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(loader), epoch_acc / len(loader)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:11.692267Z","iopub.execute_input":"2022-08-29T07:46:11.694343Z","iopub.status.idle":"2022-08-29T07:46:11.703368Z","shell.execute_reply.started":"2022-08-29T07:46:11.694281Z","shell.execute_reply":"2022-08-29T07:46:11.702039Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, loader, criterion):\n    print('')\n    print('validation to', len(loader), end=':')\n    epoch_loss = 0\n    epoch_acc = 0\n    epoch_f1 = 0\n    overall_test_output = torch.Tensor([]).to(device)\n    overall_test_labels = torch.Tensor([]).to(device)\n    \n    \n    model.eval()\n    \n    with torch.no_grad():\n        i = 0\n        for (images, labels) in loader:\n            print(i, end=';')\n            if i % 101 == 100:\n                print('')\n            i += 1\n            images = images.permute(0,3,1,2)\n            images = images.float()\n            labels = torch.reshape(labels, (-1,))\n            images = images.cuda()\n            labels = labels.cuda()\n            \n            output = model(images)\n            \n            loss = criterion(output, labels)\n            acc = calculate_accuracy(output, labels)\n            f1_score = calculate_f1_score(output, labels)\n            \n            #overall_test_output, overall_test_labels = join_batches_output(output,labels,overall_test_output,overall_test_labels)\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            epoch_f1 += f1_score.item()\n    \n    return epoch_loss / len(loader), epoch_acc / len(loader), epoch_f1 / len(loader)#, overall_test_output, overall_test_labels","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:13.120358Z","iopub.execute_input":"2022-08-29T07:46:13.120787Z","iopub.status.idle":"2022-08-29T07:46:13.130968Z","shell.execute_reply.started":"2022-08-29T07:46:13.120749Z","shell.execute_reply":"2022-08-29T07:46:13.129694Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"'''\ntrain_loss_list = []\ntrain_acc_list = []\ntrain_f1_list = []\nval_loss_list = []\nval_acc_list = []\nval_f1_list = []\ntest_loss_list = []\ntest_acc_list = []\ntest_f1_list = []\n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:46:24.410362Z","iopub.execute_input":"2022-08-29T07:46:24.410744Z","iopub.status.idle":"2022-08-29T07:46:24.416198Z","shell.execute_reply.started":"2022-08-29T07:46:24.410710Z","shell.execute_reply":"2022-08-29T07:46:24.415116Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"checkpoint = torch.load('../input/model-dict/model.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:51:36.995620Z","iopub.execute_input":"2022-08-29T07:51:36.996445Z","iopub.status.idle":"2022-08-29T07:51:37.988123Z","shell.execute_reply.started":"2022-08-29T07:51:36.996405Z","shell.execute_reply":"2022-08-29T07:51:37.987055Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 2\n'''\ntrain_loss_list = [0]*EPOCHS\ntrain_acc_list = [0]*EPOCHS\ntrain_f1_list = [0]*EPOCHS\nval_loss_list = [0]*EPOCHS\nval_acc_list = [0]*EPOCHS\nval_f1_list = [0]*EPOCHS\ntest_loss_list = [0]*EPOCHS\ntest_acc_list = [0]*EPOCHS\ntest_f1_list = [0]*EPOCHS\n'''\n\nfor epoch in range(EPOCHS):\n    print(\"Epoch-%d: \" % (epoch))\n\n    train_start_time = time.monotonic()\n    train_loss, train_acc = train(model, train_data_loader, optimizer, criterion)\n    train_end_time = time.monotonic()\n    \n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': train_loss,\n            }, './model.pt')\n\n    val_start_time = time.monotonic()\n    val_loss, val_acc, val_f1 = evaluate(model, val_data_loader, criterion)\n    val_end_time = time.monotonic()\n    \n    test_start_time = time.monotonic()\n    test_loss, test_acc, test_f1 = evaluate(model, test_data_loader, criterion)\n    test_end_time = time.monotonic()\n    \n    train_loss_list.append(train_loss)\n    train_acc_list.append(train_acc)\n    val_loss_list.append(val_loss)\n    val_acc_list.append(val_acc)\n    val_f1_list.append(val_f1)\n    test_loss_list.append(test_loss)\n    test_acc_list.append(test_acc)\n    test_f1_list.append(test_f1)\n    \n    print(\"Training: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (train_loss, train_acc, train_end_time - train_start_time))\n    print(\"Validation: Loss = %.4f, Accuracy = %.4f, f1 = %.4f, Time = %.2f seconds\" % (val_loss, val_acc, val_f1, val_end_time - val_start_time))\n    print(\"Test: Loss = %.4f, Accuracy = %.4f, f1 = %.4f, Time = %.2f seconds\" % (test_loss, test_acc, test_f1, val_end_time - val_start_time))\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2022-08-29T07:51:42.464925Z","iopub.execute_input":"2022-08-29T07:51:42.465560Z","iopub.status.idle":"2022-08-29T08:17:02.534454Z","shell.execute_reply.started":"2022-08-29T07:51:42.465524Z","shell.execute_reply":"2022-08-29T08:17:02.532803Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"img, _ = train_data.__getitem__(33000)\npredict_and_check(model, img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}