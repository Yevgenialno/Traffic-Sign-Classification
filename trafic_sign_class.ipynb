{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84149cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms,models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71da622",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "device=torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "488ab51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85c4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "learning_rate = 0.0001\n",
    "EPOCHS = 5\n",
    "numClasses = 205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a6a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8802323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficData(Dataset):\n",
    "    def __init__(self, df, image_dir, transforms=None):\n",
    "        self.image_ids = df['Path'].unique()\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index, size = [112, 112]):\n",
    "        image_path = self.image_ids[index]\n",
    "        records = self.df[self.df['Path'] == image_path]\n",
    "\n",
    "        #print(f'{self.image_dir}/{image_path}')\n",
    "        image = cv2.imread(f'{self.image_dir}/{image_path}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, size)\n",
    "        image = image.astype(float) / 255.0\n",
    "\n",
    "        target = records['ClassId'].values\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(**image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_dataset(df, dir, transform=None):\n",
    "       dataset = TrafficData(df, dir)\n",
    "       return dataset\n",
    "\n",
    "    @staticmethod \n",
    "    def loader(dataset, batch_size, num_workers=0):\n",
    "       data_loader = DataLoader(\n",
    "          dataset,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          num_workers=num_workers,\n",
    "#           collate_fn = collate_fn\n",
    "      )\n",
    "       return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44576bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"Data_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c702035",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"Data_Images/Train_balanced_data.csv\")\n",
    "df_test = pd.read_csv(\"Data_images/Test_data_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c650c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize([112, 112]),\n",
    "    transforms.ToTensor()\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c2e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TrafficData.create_dataset(df_train, path, data_transforms)\n",
    "train_data_loader = TrafficData.loader(train_data, BATCH_SIZE)\n",
    "\n",
    "test_data = TrafficData.create_dataset(df_test, path, data_transforms)\n",
    "test_data_loader = TrafficData.loader(test_data, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bee946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d926acb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 112, 112, 3])\n",
      "Shape of y: torch.Size([64, 1]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in test_data_loader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ec04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64d70156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\trafic_sign_classification\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\trafic_sign_classification\\env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "eff_net_b0 = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "class Eff_net(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Eff_net,self).__init__()\n",
    "        self.Eff_net = pretrained_model\n",
    "        #num_inputs = pretrained_model.classifier[1].in_features\n",
    "        self.fl1 = nn.Linear(1000, BATCH_SIZE)\n",
    "        self.fl2 = nn.Linear(BATCH_SIZE, numClasses)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.Eff_net(X)\n",
    "        X = F.relu(self.fl1(X))\n",
    "        X = F.dropout(X, p=0.25)\n",
    "        X = self.fl2(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec23405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Eff_net(eff_net_b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25baca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18af86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549b15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3c9d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, opt, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for (images, labels) in loader:\n",
    "        images = images.permute(0,3,1,2)\n",
    "        images = images.float()\n",
    "        labels = torch.reshape(labels, (-1,))\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        acc = calculate_accuracy(output, labels)\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8c8877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, opt, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in loader:\n",
    "            images = images.permute(0,3,1,2)\n",
    "            images = images.float()\n",
    "            labels = torch.reshape(labels, (-1,))\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            acc = calculate_accuracy(output, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce24bc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0: \n",
      "Training: Loss = 3.1380, Accuracy = 0.3614, Time = 1013.19 seconds\n",
      "Validation: Loss = 0.8786, Accuracy = 0.7945, Time = 380.50 seconds\n",
      "\n",
      "Epoch-1: \n",
      "Training: Loss = 0.4853, Accuracy = 0.8786, Time = 1050.12 seconds\n",
      "Validation: Loss = 0.3241, Accuracy = 0.9217, Time = 385.19 seconds\n",
      "\n",
      "Epoch-2: \n",
      "Training: Loss = 0.1823, Accuracy = 0.9544, Time = 1021.64 seconds\n",
      "Validation: Loss = 0.2098, Accuracy = 0.9507, Time = 356.69 seconds\n",
      "\n",
      "Epoch-3: \n",
      "Training: Loss = 0.0993, Accuracy = 0.9757, Time = 1043.61 seconds\n",
      "Validation: Loss = 0.1806, Accuracy = 0.9585, Time = 372.62 seconds\n",
      "\n",
      "Epoch-4: \n",
      "Training: Loss = 0.0666, Accuracy = 0.9837, Time = 1013.22 seconds\n",
      "Validation: Loss = 0.1549, Accuracy = 0.9664, Time = 389.84 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = [0]*EPOCHS\n",
    "train_acc_list = [0]*EPOCHS\n",
    "val_loss_list = [0]*EPOCHS\n",
    "val_acc_list = [0]*EPOCHS\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Epoch-%d: \" % (epoch))\n",
    "\n",
    "    train_start_time = time.monotonic()\n",
    "    train_loss, train_acc = train(model, train_data_loader, optimizer, criterion)\n",
    "    train_end_time = time.monotonic()\n",
    "\n",
    "    val_start_time = time.monotonic()\n",
    "    val_loss, val_acc = evaluate(model, test_data_loader, optimizer, criterion)\n",
    "    val_end_time = time.monotonic()\n",
    "    \n",
    "    train_loss_list[epoch] = train_loss\n",
    "    train_acc_list[epoch] = train_acc\n",
    "    val_loss_list[epoch] = val_loss\n",
    "    val_acc_list[epoch] = val_acc\n",
    "    \n",
    "    print(\"Training: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (train_loss, train_acc, train_end_time - train_start_time))\n",
    "    print(\"Validation: Loss = %.4f, Accuracy = %.4f, Time = %.2f seconds\" % (val_loss, val_acc, val_end_time - val_start_time))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925286c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cfb488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01da3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba0b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9922a569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffefb585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
